{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "系统找不到指定的路径。\n"
     ]
    }
   ],
   "source": [
    "!pip install --no-deps '../input/weightedboxesfusion/'> /dev/null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: omegaconf==2.0.0 from file:///D:/global-wheat-detection/input/omegaconf20/omegaconf-2.0.0-py3-none-any.whl in c:\\users\\administrator\\pytorch_cu101\\lib\\site-packages (2.0.0)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Requirement '../input/omegaconf20/omegaconf-2.0.0-py3-none-any.whl' looks like a filename, but the file does not exist\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\administrator\\pytorch_cu101\\lib\\site-packages (from omegaconf==2.0.0) (3.7.4.2)\n",
      "Requirement already satisfied: PyYAML in c:\\users\\administrator\\pytorch_cu101\\lib\\site-packages (from omegaconf==2.0.0) (5.3.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install ../input/omegaconf20/omegaconf-2.0.0-py3-none-any.whl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'cp' 不是内部或外部命令，也不是可运行的程序\n",
      "或批处理文件。\n"
     ]
    }
   ],
   "source": [
    "!cp -rf ../input/gwd-eff-code/efficientdet-pytorch-master/* ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: timm==0.1.30 from file:///D:/global-wheat-detection/input/timm0130/timm-0.1.30-py3-none-any.whl in c:\\users\\administrator\\pytorch_cu101\\lib\\site-packages (0.1.30)\n",
      "Requirement already satisfied: torch>=1.0 in c:\\users\\administrator\\pytorch_cu101\\lib\\site-packages (from timm==0.1.30) (1.5.1+cu101)\n",
      "Requirement already satisfied: torchvision in c:\\users\\administrator\\pytorch_cu101\\lib\\site-packages (from timm==0.1.30) (0.6.1+cu101)\n",
      "Requirement already satisfied: future in c:\\users\\administrator\\pytorch_cu101\\lib\\site-packages (from torch>=1.0->timm==0.1.30) (0.18.2)\n",
      "Requirement already satisfied: numpy in c:\\users\\administrator\\pytorch_cu101\\lib\\site-packages (from torch>=1.0->timm==0.1.30) (1.19.0)\n",
      "Requirement already satisfied: pillow>=4.1.1 in c:\\users\\administrator\\pytorch_cu101\\lib\\site-packages (from torchvision->timm==0.1.30) (7.2.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Requirement '../input/timm0130/timm-0.1.30-py3-none-any.whl' looks like a filename, but the file does not exist\n"
     ]
    }
   ],
   "source": [
    "!pip install ../input/timm0130/timm-0.1.30-py3-none-any.whl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Requirement '../input/effdetdependencies/pycocotools-2.0-cp37-cp37m-linux_x86_64.whl' looks like a filename, but the file does not exist\n",
      "ERROR: pycocotools-2.0-cp37-cp37m-linux_x86_64.whl is not a supported wheel on this platform.\n"
     ]
    }
   ],
   "source": [
    "!pip install ../input/effdetdependencies/pycocotools-2.0-cp37-cp37m-linux_x86_64.whl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "from ensemble_boxes import *\n",
    "import ensemble_boxes \n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from glob import glob\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "import albumentations as A\n",
    "from albumentations.pytorch.transforms import ToTensorV2\n",
    "import cv2\n",
    "import gc\n",
    "from matplotlib import pyplot as plt\n",
    "from effdet import get_efficientdet_config, EfficientDet, DetBenchEval, DetBenchTrain\n",
    "from effdet.efficientdet import HeadNet\n",
    "import os\n",
    "from datetime import datetime\n",
    "import time\n",
    "import random\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from torch.utils.data.sampler import SequentialSampler, RandomSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_WORKERS = 0 # must be 0 at windows(an unsoloved bug)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "seed_everything(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_test_transforms():\n",
    "    return A.Compose([\n",
    "            A.Resize(height=512, width=512, p=1.0),\n",
    "            ToTensorV2(p=1.0),\n",
    "        ], p=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_ROOT_PATH = r'../input/global-wheat-detection/test/'\n",
    "\n",
    "class TestDatasetRetriever(Dataset):\n",
    "\n",
    "    def __init__(self, image_ids, transforms=None):\n",
    "        super().__init__()\n",
    "        self.image_ids = image_ids\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __getitem__(self, index: int):\n",
    "        image_id = self.image_ids[index]\n",
    "        image = cv2.imread(os.path.join(DATA_ROOT_PATH,image_id+\".jpg\"), cv2.IMREAD_COLOR)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB).astype(np.float32)\n",
    "        image /= 255.0\n",
    "        if self.transforms:\n",
    "            sample = {'image': image}\n",
    "            sample = self.transforms(**sample)\n",
    "            image = sample['image']\n",
    "        return image, image_id\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return self.image_ids.shape[0]\n",
    "\n",
    "dataset = TestDatasetRetriever(\n",
    "    image_ids=np.array([path.split(os.path.sep)[-1][:-4] for path in glob(f'{DATA_ROOT_PATH}/*.jpg')]),\n",
    "    transforms=get_test_transforms()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    return tuple(zip(*batch))\n",
    "\n",
    "data_loader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=1,\n",
    "    shuffle=False,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    drop_last=False,\n",
    "    collate_fn=collate_fn\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_net(checkpoint_path,mode='eval'):\n",
    "    config = get_efficientdet_config('tf_efficientdet_d5')\n",
    "    net = EfficientDet(config, pretrained_backbone=False)\n",
    "\n",
    "    config.num_classes = 1\n",
    "    config.image_size=512\n",
    "    net.class_net = HeadNet(config, num_outputs=config.num_classes, norm_kwargs=dict(eps=.001, momentum=.01))\n",
    "\n",
    "    checkpoint = torch.load(checkpoint_path)\n",
    "    net.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "    del checkpoint\n",
    "    gc.collect()\n",
    "    if mode=='eval':\n",
    "        net = DetBenchEval(net, config)\n",
    "        net.eval()\n",
    "        return net.cuda()\n",
    "    elif mode=='train':\n",
    "        net = DetBenchTrain(net, config)\n",
    "        net.train()\n",
    "        return net\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = load_net(r'effdet5-cutmix-augmix1/best-checkpoint-035epoch.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseWheatTTA:\n",
    "    \"\"\" author: @shonenkov \"\"\"\n",
    "    image_size = 512\n",
    "\n",
    "    def augment(self, image):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def batch_augment(self, images):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def deaugment_boxes(self, boxes):\n",
    "        raise NotImplementedError\n",
    "\n",
    "\n",
    "class TTAHorizontalFlip(BaseWheatTTA):\n",
    "    \"\"\" author: @shonenkov \"\"\"\n",
    "\n",
    "    def augment(self, image):\n",
    "        return image.flip(1)\n",
    "\n",
    "    def batch_augment(self, images):\n",
    "        return images.flip(2)\n",
    "\n",
    "    def deaugment_boxes(self, boxes):\n",
    "        boxes[:, [1, 3]] = self.image_size - boxes[:, [3, 1]]\n",
    "        return boxes\n",
    "\n",
    "\n",
    "class TTAVerticalFlip(BaseWheatTTA):\n",
    "    \"\"\" author: @shonenkov \"\"\"\n",
    "\n",
    "    def augment(self, image):\n",
    "        return image.flip(2)\n",
    "\n",
    "    def batch_augment(self, images):\n",
    "        return images.flip(3)\n",
    "\n",
    "    def deaugment_boxes(self, boxes):\n",
    "        boxes[:, [0, 2]] = self.image_size - boxes[:, [2, 0]]\n",
    "        return boxes\n",
    "\n",
    "\n",
    "class TTARotate90(BaseWheatTTA):\n",
    "    \"\"\" author: @shonenkov \"\"\"\n",
    "\n",
    "    def augment(self, image):\n",
    "        return torch.rot90(image, 1, (1, 2))\n",
    "\n",
    "    def batch_augment(self, images):\n",
    "        return torch.rot90(images, 1, (2, 3))\n",
    "\n",
    "    def deaugment_boxes(self, boxes):\n",
    "        res_boxes = boxes.copy()\n",
    "        res_boxes[:, [0, 2]] = self.image_size - boxes[:, [1, 3]]\n",
    "        res_boxes[:, [1, 3]] = boxes[:, [2, 0]]\n",
    "        return res_boxes\n",
    "\n",
    "\n",
    "class TTACompose(BaseWheatTTA):\n",
    "    \"\"\" author: @shonenkov \"\"\"\n",
    "\n",
    "    def __init__(self, transforms):\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def augment(self, image):\n",
    "        for transform in self.transforms:\n",
    "            image = transform.augment(image)\n",
    "        return image\n",
    "\n",
    "    def batch_augment(self, images):\n",
    "        for transform in self.transforms:\n",
    "            images = transform.batch_augment(images)\n",
    "        return images\n",
    "\n",
    "    def prepare_boxes(self, boxes):\n",
    "        result_boxes = boxes.copy()\n",
    "        result_boxes[:, 0] = np.min(boxes[:, [0, 2]], axis=1)\n",
    "        result_boxes[:, 2] = np.max(boxes[:, [0, 2]], axis=1)\n",
    "        result_boxes[:, 1] = np.min(boxes[:, [1, 3]], axis=1)\n",
    "        result_boxes[:, 3] = np.max(boxes[:, [1, 3]], axis=1)\n",
    "        return result_boxes\n",
    "\n",
    "    def deaugment_boxes(self, boxes):\n",
    "        for transform in self.transforms[::-1]:\n",
    "            boxes = transform.deaugment_boxes(boxes)\n",
    "        return self.prepare_boxes(boxes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "\n",
    "tta_transforms = []\n",
    "for tta_combination in product([TTAHorizontalFlip(), None],\n",
    "                               [TTAVerticalFlip(), None],\n",
    "                               [TTARotate90(), None]):\n",
    "    tta_transforms.append(TTACompose([tta_transform for tta_transform in tta_combination if tta_transform]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_tta_predictions(images, score_threshold=0.25):\n",
    "    with torch.no_grad():\n",
    "        images = torch.stack(images).float().cuda()\n",
    "        predictions = []\n",
    "        for tta_transform in tta_transforms:\n",
    "            result = []\n",
    "            det = net(tta_transform.batch_augment(images.clone()), torch.tensor([1] * images.shape[0]).float().cuda())\n",
    "\n",
    "            for i in range(images.shape[0]):\n",
    "                boxes = det[i].detach().cpu().numpy()[:, :4]\n",
    "                scores = det[i].detach().cpu().numpy()[:, 4]\n",
    "                indexes = np.where(scores > score_threshold)[0]\n",
    "                boxes = boxes[indexes]\n",
    "                boxes[:, 2] = boxes[:, 2] + boxes[:, 0]\n",
    "                boxes[:, 3] = boxes[:, 3] + boxes[:, 1]\n",
    "                boxes = tta_transform.deaugment_boxes(boxes.copy())\n",
    "                result.append({\n",
    "                    'boxes': boxes,\n",
    "                    'scores': scores[indexes],\n",
    "                })\n",
    "            predictions.append(result)\n",
    "    return predictions\n",
    "\n",
    "def run_wbf(predictions, image_index, image_size=512, iou_thr=0.44, skip_box_thr=0.43, weights=None):\n",
    "    boxes = [(prediction[image_index]['boxes']/(image_size-1)).tolist()  for prediction in predictions]\n",
    "    scores = [prediction[image_index]['scores'].tolist()  for prediction in predictions]\n",
    "    labels = [np.ones(prediction[image_index]['scores'].shape[0]).tolist() for prediction in predictions]\n",
    "    boxes, scores, labels = weighted_boxes_fusion(boxes, scores, labels, weights=None, iou_thr=iou_thr, skip_box_thr=skip_box_thr)\n",
    "    boxes = boxes*(image_size-1)\n",
    "    return boxes, scores, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**============TTA prediction================**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image ids: ('2fd875eaa',)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "..\\aten\\src\\ATen\\native\\BinaryOps.cpp:81: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image ids: ('348a992bb',)\n",
      "image ids: ('51b3e36ab',)\n",
      "image ids: ('51f1be19e',)\n",
      "image ids: ('53f253011',)\n",
      "image ids: ('796707dd7',)\n",
      "image ids: ('aac893a91',)\n",
      "image ids: ('cb8d261a3',)\n",
      "image ids: ('cc3532ff6',)\n",
      "image ids: ('f5a1f0358',)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_id</th>\n",
       "      <th>width</th>\n",
       "      <th>height</th>\n",
       "      <th>source</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>w</th>\n",
       "      <th>h</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2fd875eaa</td>\n",
       "      <td>1024</td>\n",
       "      <td>1024</td>\n",
       "      <td>usask_1</td>\n",
       "      <td>0</td>\n",
       "      <td>910</td>\n",
       "      <td>105</td>\n",
       "      <td>83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2fd875eaa</td>\n",
       "      <td>1024</td>\n",
       "      <td>1024</td>\n",
       "      <td>usask_1</td>\n",
       "      <td>106</td>\n",
       "      <td>584</td>\n",
       "      <td>141</td>\n",
       "      <td>85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2fd875eaa</td>\n",
       "      <td>1024</td>\n",
       "      <td>1024</td>\n",
       "      <td>usask_1</td>\n",
       "      <td>731</td>\n",
       "      <td>886</td>\n",
       "      <td>97</td>\n",
       "      <td>90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2fd875eaa</td>\n",
       "      <td>1024</td>\n",
       "      <td>1024</td>\n",
       "      <td>usask_1</td>\n",
       "      <td>533</td>\n",
       "      <td>868</td>\n",
       "      <td>109</td>\n",
       "      <td>118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2fd875eaa</td>\n",
       "      <td>1024</td>\n",
       "      <td>1024</td>\n",
       "      <td>usask_1</td>\n",
       "      <td>458</td>\n",
       "      <td>496</td>\n",
       "      <td>81</td>\n",
       "      <td>136</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    image_id  width  height   source    x    y    w    h\n",
       "0  2fd875eaa   1024    1024  usask_1    0  910  105   83\n",
       "1  2fd875eaa   1024    1024  usask_1  106  584  141   85\n",
       "2  2fd875eaa   1024    1024  usask_1  731  886   97   90\n",
       "3  2fd875eaa   1024    1024  usask_1  533  868  109  118\n",
       "4  2fd875eaa   1024    1024  usask_1  458  496   81  136"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## generate plabel\n",
    "\n",
    "results_plabel = []\n",
    "\n",
    "for j, (images, image_ids) in enumerate(data_loader):\n",
    "    print(\"image ids:\",image_ids)\n",
    "    predictions = make_tta_predictions(images)\n",
    "    #predictions = make_predictions(images)\n",
    "    for i,image in enumerate(images):\n",
    "        i = 0\n",
    "        sample = images[i].permute(1, 2, 0).cpu().numpy()\n",
    "\n",
    "        boxes, scores, labels = run_wbf(predictions, image_index=i)\n",
    "\n",
    "        #boxes = boxes.astype(np.int32).clip(min=0, max=511)\n",
    "\n",
    "        boxes = (boxes * 2).astype(np.int32).clip(min=0, max=1023) # original image size is 1024, resize to 512 during training\n",
    "\n",
    "        image_id = image_ids[i]\n",
    "\n",
    "        boxes[:, 2] = boxes[:, 2] - boxes[:, 0]\n",
    "        boxes[:, 3] = boxes[:, 3] - boxes[:, 1]\n",
    "        w,h = 1024,1024\n",
    "        for box in boxes:\n",
    "            result_p = {\n",
    "                'image_id': image_id,\n",
    "                'width':w,\n",
    "                'height':h,\n",
    "                'source':'usask_1',\n",
    "                'x':box[0],\n",
    "                'y':box[1],\n",
    "                'w':box[2],\n",
    "                'h':box[3],\n",
    "            }\n",
    "            results_plabel.append(result_p)\n",
    "results_df = pd.DataFrame(results_plabel, columns=['image_id', 'width','height','source','x','y','w','h'])\n",
    "results_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "****==================pesudo Labeling==================****"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_id</th>\n",
       "      <th>width</th>\n",
       "      <th>height</th>\n",
       "      <th>source</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>w</th>\n",
       "      <th>h</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>../input/global-wheat-detection/test//2fd875ea...</td>\n",
       "      <td>1024</td>\n",
       "      <td>1024</td>\n",
       "      <td>usask_1</td>\n",
       "      <td>0</td>\n",
       "      <td>910</td>\n",
       "      <td>105</td>\n",
       "      <td>83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>../input/global-wheat-detection/test//2fd875ea...</td>\n",
       "      <td>1024</td>\n",
       "      <td>1024</td>\n",
       "      <td>usask_1</td>\n",
       "      <td>106</td>\n",
       "      <td>584</td>\n",
       "      <td>141</td>\n",
       "      <td>85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>../input/global-wheat-detection/test//2fd875ea...</td>\n",
       "      <td>1024</td>\n",
       "      <td>1024</td>\n",
       "      <td>usask_1</td>\n",
       "      <td>731</td>\n",
       "      <td>886</td>\n",
       "      <td>97</td>\n",
       "      <td>90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>../input/global-wheat-detection/test//2fd875ea...</td>\n",
       "      <td>1024</td>\n",
       "      <td>1024</td>\n",
       "      <td>usask_1</td>\n",
       "      <td>533</td>\n",
       "      <td>868</td>\n",
       "      <td>109</td>\n",
       "      <td>118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>../input/global-wheat-detection/test//2fd875ea...</td>\n",
       "      <td>1024</td>\n",
       "      <td>1024</td>\n",
       "      <td>usask_1</td>\n",
       "      <td>458</td>\n",
       "      <td>496</td>\n",
       "      <td>81</td>\n",
       "      <td>136</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            image_id  width  height   source  \\\n",
       "0  ../input/global-wheat-detection/test//2fd875ea...   1024    1024  usask_1   \n",
       "1  ../input/global-wheat-detection/test//2fd875ea...   1024    1024  usask_1   \n",
       "2  ../input/global-wheat-detection/test//2fd875ea...   1024    1024  usask_1   \n",
       "3  ../input/global-wheat-detection/test//2fd875ea...   1024    1024  usask_1   \n",
       "4  ../input/global-wheat-detection/test//2fd875ea...   1024    1024  usask_1   \n",
       "\n",
       "     x    y    w    h  \n",
       "0    0  910  105   83  \n",
       "1  106  584  141   85  \n",
       "2  731  886   97   90  \n",
       "3  533  868  109  118  \n",
       "4  458  496   81  136  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df['image_id'] = results_df['image_id'].apply(lambda x: DATA_ROOT_PATH+'/'+ x+'.jpg')\n",
    "results_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "marking = pd.read_csv('../input/global-wheat-detection/train.csv')\n",
    "\n",
    "bboxs = np.stack(marking['bbox'].apply(lambda x: np.fromstring(x[1:-1], sep=',')))\n",
    "for i, column in enumerate(['x', 'y', 'w', 'h']):\n",
    "    marking[column] = bboxs[:,i]\n",
    "marking.drop(columns=['bbox'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_id</th>\n",
       "      <th>width</th>\n",
       "      <th>height</th>\n",
       "      <th>source</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>w</th>\n",
       "      <th>h</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>b6ab77fd7</td>\n",
       "      <td>1024</td>\n",
       "      <td>1024</td>\n",
       "      <td>usask_1</td>\n",
       "      <td>834.0</td>\n",
       "      <td>222.0</td>\n",
       "      <td>56.0</td>\n",
       "      <td>36.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>b6ab77fd7</td>\n",
       "      <td>1024</td>\n",
       "      <td>1024</td>\n",
       "      <td>usask_1</td>\n",
       "      <td>226.0</td>\n",
       "      <td>548.0</td>\n",
       "      <td>130.0</td>\n",
       "      <td>58.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>b6ab77fd7</td>\n",
       "      <td>1024</td>\n",
       "      <td>1024</td>\n",
       "      <td>usask_1</td>\n",
       "      <td>377.0</td>\n",
       "      <td>504.0</td>\n",
       "      <td>74.0</td>\n",
       "      <td>160.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>b6ab77fd7</td>\n",
       "      <td>1024</td>\n",
       "      <td>1024</td>\n",
       "      <td>usask_1</td>\n",
       "      <td>834.0</td>\n",
       "      <td>95.0</td>\n",
       "      <td>109.0</td>\n",
       "      <td>107.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>b6ab77fd7</td>\n",
       "      <td>1024</td>\n",
       "      <td>1024</td>\n",
       "      <td>usask_1</td>\n",
       "      <td>26.0</td>\n",
       "      <td>144.0</td>\n",
       "      <td>124.0</td>\n",
       "      <td>117.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    image_id  width  height   source      x      y      w      h\n",
       "0  b6ab77fd7   1024    1024  usask_1  834.0  222.0   56.0   36.0\n",
       "1  b6ab77fd7   1024    1024  usask_1  226.0  548.0  130.0   58.0\n",
       "2  b6ab77fd7   1024    1024  usask_1  377.0  504.0   74.0  160.0\n",
       "3  b6ab77fd7   1024    1024  usask_1  834.0   95.0  109.0  107.0\n",
       "4  b6ab77fd7   1024    1024  usask_1   26.0  144.0  124.0  117.0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "marking.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_ROOT_PATH = '../input/global-wheat-detection/train'\n",
    "marking['image_id'] = marking['image_id'].apply(lambda x: TRAIN_ROOT_PATH+'/'+ x+'.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_id</th>\n",
       "      <th>width</th>\n",
       "      <th>height</th>\n",
       "      <th>source</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>w</th>\n",
       "      <th>h</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>../input/global-wheat-detection/train/b6ab77fd...</td>\n",
       "      <td>1024</td>\n",
       "      <td>1024</td>\n",
       "      <td>usask_1</td>\n",
       "      <td>834.0</td>\n",
       "      <td>222.0</td>\n",
       "      <td>56.0</td>\n",
       "      <td>36.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>../input/global-wheat-detection/train/b6ab77fd...</td>\n",
       "      <td>1024</td>\n",
       "      <td>1024</td>\n",
       "      <td>usask_1</td>\n",
       "      <td>226.0</td>\n",
       "      <td>548.0</td>\n",
       "      <td>130.0</td>\n",
       "      <td>58.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>../input/global-wheat-detection/train/b6ab77fd...</td>\n",
       "      <td>1024</td>\n",
       "      <td>1024</td>\n",
       "      <td>usask_1</td>\n",
       "      <td>377.0</td>\n",
       "      <td>504.0</td>\n",
       "      <td>74.0</td>\n",
       "      <td>160.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>../input/global-wheat-detection/train/b6ab77fd...</td>\n",
       "      <td>1024</td>\n",
       "      <td>1024</td>\n",
       "      <td>usask_1</td>\n",
       "      <td>834.0</td>\n",
       "      <td>95.0</td>\n",
       "      <td>109.0</td>\n",
       "      <td>107.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>../input/global-wheat-detection/train/b6ab77fd...</td>\n",
       "      <td>1024</td>\n",
       "      <td>1024</td>\n",
       "      <td>usask_1</td>\n",
       "      <td>26.0</td>\n",
       "      <td>144.0</td>\n",
       "      <td>124.0</td>\n",
       "      <td>117.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            image_id  width  height   source  \\\n",
       "0  ../input/global-wheat-detection/train/b6ab77fd...   1024    1024  usask_1   \n",
       "1  ../input/global-wheat-detection/train/b6ab77fd...   1024    1024  usask_1   \n",
       "2  ../input/global-wheat-detection/train/b6ab77fd...   1024    1024  usask_1   \n",
       "3  ../input/global-wheat-detection/train/b6ab77fd...   1024    1024  usask_1   \n",
       "4  ../input/global-wheat-detection/train/b6ab77fd...   1024    1024  usask_1   \n",
       "\n",
       "       x      y      w      h  \n",
       "0  834.0  222.0   56.0   36.0  \n",
       "1  226.0  548.0  130.0   58.0  \n",
       "2  377.0  504.0   74.0  160.0  \n",
       "3  834.0   95.0  109.0  107.0  \n",
       "4   26.0  144.0  124.0  117.0  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "marking.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_id</th>\n",
       "      <th>width</th>\n",
       "      <th>height</th>\n",
       "      <th>source</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>w</th>\n",
       "      <th>h</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>../input/global-wheat-detection/test//2fd875ea...</td>\n",
       "      <td>1024</td>\n",
       "      <td>1024</td>\n",
       "      <td>usask_1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>910.0</td>\n",
       "      <td>105.0</td>\n",
       "      <td>83.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>../input/global-wheat-detection/test//2fd875ea...</td>\n",
       "      <td>1024</td>\n",
       "      <td>1024</td>\n",
       "      <td>usask_1</td>\n",
       "      <td>106.0</td>\n",
       "      <td>584.0</td>\n",
       "      <td>141.0</td>\n",
       "      <td>85.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>../input/global-wheat-detection/test//2fd875ea...</td>\n",
       "      <td>1024</td>\n",
       "      <td>1024</td>\n",
       "      <td>usask_1</td>\n",
       "      <td>731.0</td>\n",
       "      <td>886.0</td>\n",
       "      <td>97.0</td>\n",
       "      <td>90.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>../input/global-wheat-detection/test//2fd875ea...</td>\n",
       "      <td>1024</td>\n",
       "      <td>1024</td>\n",
       "      <td>usask_1</td>\n",
       "      <td>533.0</td>\n",
       "      <td>868.0</td>\n",
       "      <td>109.0</td>\n",
       "      <td>118.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>../input/global-wheat-detection/test//2fd875ea...</td>\n",
       "      <td>1024</td>\n",
       "      <td>1024</td>\n",
       "      <td>usask_1</td>\n",
       "      <td>458.0</td>\n",
       "      <td>496.0</td>\n",
       "      <td>81.0</td>\n",
       "      <td>136.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            image_id  width  height   source  \\\n",
       "0  ../input/global-wheat-detection/test//2fd875ea...   1024    1024  usask_1   \n",
       "1  ../input/global-wheat-detection/test//2fd875ea...   1024    1024  usask_1   \n",
       "2  ../input/global-wheat-detection/test//2fd875ea...   1024    1024  usask_1   \n",
       "3  ../input/global-wheat-detection/test//2fd875ea...   1024    1024  usask_1   \n",
       "4  ../input/global-wheat-detection/test//2fd875ea...   1024    1024  usask_1   \n",
       "\n",
       "       x      y      w      h  \n",
       "0    0.0  910.0  105.0   83.0  \n",
       "1  106.0  584.0  141.0   85.0  \n",
       "2  731.0  886.0   97.0   90.0  \n",
       "3  533.0  868.0  109.0  118.0  \n",
       "4  458.0  496.0   81.0  136.0  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_plabel = pd.concat([results_df,marking], axis=0)\n",
    "train_data_plabel.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bbox_count</th>\n",
       "      <th>source</th>\n",
       "      <th>stratify_group</th>\n",
       "      <th>fold</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>image_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>../input/global-wheat-detection/test//2fd875eaa.jpg</th>\n",
       "      <td>24</td>\n",
       "      <td>NaN</td>\n",
       "      <td>nan_1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>../input/global-wheat-detection/test//348a992bb.jpg</th>\n",
       "      <td>37</td>\n",
       "      <td>NaN</td>\n",
       "      <td>nan_2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>../input/global-wheat-detection/test//51b3e36ab.jpg</th>\n",
       "      <td>25</td>\n",
       "      <td>NaN</td>\n",
       "      <td>nan_1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>../input/global-wheat-detection/test//51f1be19e.jpg</th>\n",
       "      <td>18</td>\n",
       "      <td>NaN</td>\n",
       "      <td>nan_1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>../input/global-wheat-detection/test//53f253011.jpg</th>\n",
       "      <td>31</td>\n",
       "      <td>NaN</td>\n",
       "      <td>nan_2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    bbox_count source  \\\n",
       "image_id                                                                \n",
       "../input/global-wheat-detection/test//2fd875eaa...          24    NaN   \n",
       "../input/global-wheat-detection/test//348a992bb...          37    NaN   \n",
       "../input/global-wheat-detection/test//51b3e36ab...          25    NaN   \n",
       "../input/global-wheat-detection/test//51f1be19e...          18    NaN   \n",
       "../input/global-wheat-detection/test//53f253011...          31    NaN   \n",
       "\n",
       "                                                   stratify_group  fold  \n",
       "image_id                                                                 \n",
       "../input/global-wheat-detection/test//2fd875eaa...          nan_1     0  \n",
       "../input/global-wheat-detection/test//348a992bb...          nan_2     0  \n",
       "../input/global-wheat-detection/test//51b3e36ab...          nan_1     0  \n",
       "../input/global-wheat-detection/test//51f1be19e...          nan_1     0  \n",
       "../input/global-wheat-detection/test//53f253011...          nan_2     0  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_folds = train_data_plabel[['image_id']].copy()\n",
    "df_folds.loc[:, 'bbox_count'] = 1\n",
    "df_folds = df_folds.groupby('image_id').count()\n",
    "df_folds.loc[:, 'source'] = marking[['image_id', 'source']].groupby('image_id').min()['source']\n",
    "df_folds.loc[:, 'stratify_group'] = np.char.add(\n",
    "    df_folds['source'].values.astype(str),\n",
    "    df_folds['bbox_count'].apply(lambda x: f'_{x // 15}').values.astype(str)\n",
    ")\n",
    "df_folds.loc[:, 'fold'] = 0\n",
    "df_folds.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\administrator\\pytorch_cu101\\lib\\site-packages\\sklearn\\model_selection\\_split.py:657: Warning: The least populated class in y has only 1 members, which is too few. The minimum number of members in any class cannot be less than n_splits=5.\n",
      "  % (min_groups, self.n_splits)), Warning)\n"
     ]
    }
   ],
   "source": [
    "#K-Fold preparation\n",
    "for fold_number, (train_index, val_index) in enumerate(skf.split(X=df_folds.index, y=df_folds['stratify_group'])):\n",
    "    df_folds.loc[df_folds.iloc[val_index].index, 'fold'] = fold_number\n",
    "    print(fold_number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetRetriever(Dataset):\n",
    "\n",
    "    def __init__(self, marking, image_ids, transforms=None, test=False):\n",
    "        super().__init__()\n",
    "        self.image_ids = image_ids\n",
    "        self.marking = marking\n",
    "        self.transforms = transforms\n",
    "        self.test = test\n",
    "\n",
    "    def __getitem__(self, index: int):\n",
    "        image_id = self.image_ids[index]\n",
    "        \n",
    "        if self.test or random.random() > 0.5:\n",
    "            image, boxes = self.load_image_and_boxes(index)\n",
    "        else:\n",
    "            image, boxes = self.load_cutmix_image_and_boxes(index)\n",
    "\n",
    "        # there is only one class\n",
    "        labels = torch.ones((boxes.shape[0],), dtype=torch.int64)\n",
    "        \n",
    "        target = {}\n",
    "        target['boxes'] = boxes\n",
    "        target['labels'] = labels\n",
    "        target['image_id'] = torch.tensor([index])\n",
    "\n",
    "        if self.transforms:\n",
    "            for i in range(10):\n",
    "                sample = self.transforms(**{\n",
    "                    'image': image,\n",
    "                    'bboxes': target['boxes'],\n",
    "                    'labels': labels\n",
    "                })\n",
    "                if len(sample['bboxes']) > 0:\n",
    "                    image = sample['image']\n",
    "                    target['boxes'] = torch.stack(tuple(map(torch.tensor, zip(*sample['bboxes'])))).permute(1, 0)\n",
    "                    target['boxes'][:,[0,1,2,3]] = target['boxes'][:,[1,0,3,2]]  #yxyx: be warning\n",
    "                    break\n",
    "\n",
    "        return image, target, image_id\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return self.image_ids.shape[0]\n",
    "\n",
    "    def load_image_and_boxes(self, index):\n",
    "        image_id = self.image_ids[index]\n",
    "        image = cv2.imread(image_id, cv2.IMREAD_COLOR)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB).astype(np.float32)\n",
    "        image /= 255.0\n",
    "        records = self.marking[self.marking['image_id'] == image_id]\n",
    "        boxes = records[['x', 'y', 'w', 'h']].values\n",
    "        boxes[:, 2] = boxes[:, 0] + boxes[:, 2]\n",
    "        boxes[:, 3] = boxes[:, 1] + boxes[:, 3]\n",
    "        return image, boxes\n",
    "\n",
    "    def load_cutmix_image_and_boxes(self, index, imsize=1024):\n",
    "        \"\"\" \n",
    "        This implementation of cutmix author:  https://www.kaggle.com/nvnnghia \n",
    "        Refactoring and adaptation: https://www.kaggle.com/shonenkov\n",
    "        \"\"\"\n",
    "        w, h = imsize, imsize\n",
    "        s = imsize // 2\n",
    "    \n",
    "        xc, yc = [int(random.uniform(imsize * 0.25, imsize * 0.75)) for _ in range(2)]  # center x, y\n",
    "        indexes = [index] + [random.randint(0, self.image_ids.shape[0] - 1) for _ in range(3)]\n",
    "\n",
    "        result_image = np.full((imsize, imsize, 3), 1, dtype=np.float32)\n",
    "        result_boxes = []\n",
    "\n",
    "        for i, index in enumerate(indexes):\n",
    "            image, boxes = self.load_image_and_boxes(index)\n",
    "            if i == 0:\n",
    "                x1a, y1a, x2a, y2a = max(xc - w, 0), max(yc - h, 0), xc, yc  # xmin, ymin, xmax, ymax (large image)\n",
    "                x1b, y1b, x2b, y2b = w - (x2a - x1a), h - (y2a - y1a), w, h  # xmin, ymin, xmax, ymax (small image)\n",
    "            elif i == 1:  # top right\n",
    "                x1a, y1a, x2a, y2a = xc, max(yc - h, 0), min(xc + w, s * 2), yc\n",
    "                x1b, y1b, x2b, y2b = 0, h - (y2a - y1a), min(w, x2a - x1a), h\n",
    "            elif i == 2:  # bottom left\n",
    "                x1a, y1a, x2a, y2a = max(xc - w, 0), yc, xc, min(s * 2, yc + h)\n",
    "                x1b, y1b, x2b, y2b = w - (x2a - x1a), 0, max(xc, w), min(y2a - y1a, h)\n",
    "            elif i == 3:  # bottom right\n",
    "                x1a, y1a, x2a, y2a = xc, yc, min(xc + w, s * 2), min(s * 2, yc + h)\n",
    "                x1b, y1b, x2b, y2b = 0, 0, min(w, x2a - x1a), min(y2a - y1a, h)\n",
    "            result_image[y1a:y2a, x1a:x2a] = image[y1b:y2b, x1b:x2b]\n",
    "            padw = x1a - x1b\n",
    "            padh = y1a - y1b\n",
    "\n",
    "            boxes[:, 0] += padw\n",
    "            boxes[:, 1] += padh\n",
    "            boxes[:, 2] += padw\n",
    "            boxes[:, 3] += padh\n",
    "\n",
    "            result_boxes.append(boxes)\n",
    "\n",
    "        result_boxes = np.concatenate(result_boxes, 0)\n",
    "        np.clip(result_boxes[:, 0:], 0, 2 * s, out=result_boxes[:, 0:])\n",
    "        result_boxes = result_boxes.astype(np.int32)\n",
    "        result_boxes = result_boxes[np.where((result_boxes[:,2]-result_boxes[:,0])*(result_boxes[:,3]-result_boxes[:,1]) > 0)]\n",
    "        return result_image, result_boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_transforms():\n",
    "    return A.Compose(\n",
    "        [\n",
    "            A.RandomSizedCrop(min_max_height=(800, 800), height=512, width=512, p=0.5),\n",
    "            A.OneOf([\n",
    "                A.HueSaturationValue(hue_shift_limit=0.2, sat_shift_limit= 0.2, \n",
    "                                     val_shift_limit=0.2, p=0.9),\n",
    "                A.RandomBrightnessContrast(brightness_limit=0.2, \n",
    "                                           contrast_limit=0.2, p=0.9),\n",
    "            ],p=0.9),\n",
    "            A.ToGray(p=0.01),\n",
    "            A.HorizontalFlip(p=0.5),\n",
    "            A.VerticalFlip(p=0.5),\n",
    "            A.Resize(height=512, width=512, p=1),\n",
    "            A.Cutout(num_holes=8, max_h_size=64, max_w_size=64, fill_value=0, p=0.5),\n",
    "            ToTensorV2(p=1.0),\n",
    "        ], \n",
    "        p=1.0, \n",
    "        bbox_params=A.BboxParams(\n",
    "            format='pascal_voc',\n",
    "            min_area=0, \n",
    "            min_visibility=0,\n",
    "            label_fields=['labels']\n",
    "        )\n",
    "    )\n",
    "\n",
    "def get_valid_transforms():\n",
    "    return A.Compose(\n",
    "        [\n",
    "            A.Resize(height=512, width=512, p=1.0),\n",
    "            ToTensorV2(p=1.0),\n",
    "        ], \n",
    "        p=1.0, \n",
    "        bbox_params=A.BboxParams(\n",
    "            format='pascal_voc',\n",
    "            min_area=0, \n",
    "            min_visibility=0,\n",
    "            label_fields=['labels']\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "fold_number = 0\n",
    "\n",
    "train_dataset = DatasetRetriever(\n",
    "    image_ids=df_folds[df_folds['fold'] != fold_number].index.values,\n",
    "    marking=train_data_plabel,\n",
    "    transforms=get_train_transforms(),\n",
    "    test=False,\n",
    ")\n",
    "\n",
    "validation_dataset = DatasetRetriever(\n",
    "    image_ids=df_folds[df_folds['fold'] == fold_number].index.values,\n",
    "    marking=train_data_plabel,\n",
    "    transforms=get_valid_transforms(),\n",
    "    test=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    return tuple(zip(*batch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "class Fitter:\n",
    "    \n",
    "    def __init__(self, model, device, config):\n",
    "        self.config = config\n",
    "        self.epoch = 0\n",
    "\n",
    "        self.base_dir = f'./{config.folder}'\n",
    "        if not os.path.exists(self.base_dir):\n",
    "            os.makedirs(self.base_dir)\n",
    "        \n",
    "        self.log_path = f'{self.base_dir}/log.txt'\n",
    "        self.best_summary_loss = 10**5\n",
    "\n",
    "        self.model = model\n",
    "        self.device = device\n",
    "\n",
    "        param_optimizer = list(self.model.named_parameters())\n",
    "        no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n",
    "        optimizer_grouped_parameters = [\n",
    "            {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.001},\n",
    "            {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "        ] \n",
    "\n",
    "        self.optimizer = torch.optim.AdamW(self.model.parameters(), lr=config.lr)\n",
    "        self.scheduler = config.SchedulerClass(self.optimizer, **config.scheduler_params)\n",
    "        self.log(f'Fitter prepared. Device is {self.device}')\n",
    "\n",
    "    def fit(self, train_loader, validation_loader):\n",
    "        for e in range(self.config.n_epochs):\n",
    "            if self.config.verbose:\n",
    "                lr = self.optimizer.param_groups[0]['lr']\n",
    "                timestamp = datetime.utcnow().isoformat()\n",
    "                self.log(f'\\n{timestamp}\\nLR: {lr}')\n",
    "\n",
    "            t = time.time()\n",
    "            summary_loss = self.train_one_epoch(train_loader)\n",
    "\n",
    "            self.log(f'[RESULT]: Train. Epoch: {self.epoch}, summary_loss: {summary_loss.avg:.5f}, time: {(time.time() - t):.5f}')\n",
    "            self.save(f'{self.base_dir}/last-checkpoint1.bin')\n",
    "            print(f'saving: {self.base_dir}/last-checkpoint1.bin...')\n",
    "\n",
    "            t = time.time()\n",
    "            summary_loss = self.validation(validation_loader)\n",
    "\n",
    "            self.log(f'[RESULT]: Val. Epoch: {self.epoch}, summary_loss: {summary_loss.avg:.5f}, time: {(time.time() - t):.5f}')\n",
    "            if summary_loss.avg < self.best_summary_loss:\n",
    "                self.best_summary_loss = summary_loss.avg\n",
    "                self.model.eval()\n",
    "                self.save(f'{self.base_dir}/best-checkpoint-{str(self.epoch).zfill(3)}epoch.bin')\n",
    "                for path in sorted(glob(f'{self.base_dir}/best-checkpoint-*epoch.bin'))[:-3]:\n",
    "                    os.remove(path)\n",
    "\n",
    "            if self.config.validation_scheduler:\n",
    "                self.scheduler.step(metrics=summary_loss.avg)\n",
    "\n",
    "            self.epoch += 1\n",
    "\n",
    "    def validation(self, val_loader):\n",
    "        self.model.eval()\n",
    "        summary_loss = AverageMeter()\n",
    "        t = time.time()\n",
    "        for step, (images, targets, image_ids) in enumerate(val_loader):\n",
    "            if self.config.verbose:\n",
    "                if step % self.config.verbose_step == 0:\n",
    "                    print(\n",
    "                        f'Val Step {step}/{len(val_loader)}, ' + \\\n",
    "                        f'summary_loss: {summary_loss.avg:.5f}, ' + \\\n",
    "                        f'time: {(time.time() - t):.5f}', end='\\r'\n",
    "                    )\n",
    "            with torch.no_grad():\n",
    "                images = torch.stack(images)\n",
    "                batch_size = images.shape[0]\n",
    "                images = images.to(self.device).float()\n",
    "                boxes = [target['boxes'].to(self.device).float() for target in targets]\n",
    "                labels = [target['labels'].to(self.device).float() for target in targets]\n",
    "\n",
    "                loss, _, _ = self.model(images, boxes, labels)\n",
    "                summary_loss.update(loss.detach().item(), batch_size)\n",
    "\n",
    "        return summary_loss\n",
    "\n",
    "    def train_one_epoch(self, train_loader):\n",
    "        self.model.train()\n",
    "        summary_loss = AverageMeter()\n",
    "        t = time.time()\n",
    "        for step, (images, targets, image_ids) in enumerate(train_loader):\n",
    "            if self.config.verbose:\n",
    "                if step % self.config.verbose_step == 0:\n",
    "                    print(\n",
    "                        f'Train Step {step}/{len(train_loader)}, ' + \\\n",
    "                        f'summary_loss: {summary_loss.avg:.5f}, ' + \\\n",
    "                        f'time: {(time.time() - t):.5f}', end='\\r'\n",
    "                    )\n",
    "            \n",
    "            images = torch.stack(images)\n",
    "            images = images.to(self.device).float()\n",
    "            batch_size = images.shape[0]\n",
    "            boxes = [target['boxes'].to(self.device).float() for target in targets]\n",
    "            labels = [target['labels'].to(self.device).float() for target in targets]\n",
    "\n",
    "            self.optimizer.zero_grad()\n",
    "            \n",
    "            loss, _, _ = self.model(images, boxes, labels)\n",
    "            \n",
    "            loss.backward()\n",
    "\n",
    "            summary_loss.update(loss.detach().item(), batch_size)\n",
    "\n",
    "            self.optimizer.step()\n",
    "\n",
    "            if self.config.step_scheduler:\n",
    "                self.scheduler.step()\n",
    "\n",
    "        return summary_loss\n",
    "    \n",
    "    def save(self, path):\n",
    "        self.model.eval()\n",
    "        torch.save({\n",
    "            'model_state_dict': self.model.model.state_dict(),\n",
    "            'optimizer_state_dict': self.optimizer.state_dict(),\n",
    "            'scheduler_state_dict': self.scheduler.state_dict(),\n",
    "            'best_summary_loss': self.best_summary_loss,\n",
    "            'epoch': self.epoch,\n",
    "        }, path)\n",
    "\n",
    "    def load(self, path):\n",
    "        checkpoint = torch.load(path)\n",
    "        self.model.model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        self.scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
    "        self.best_summary_loss = checkpoint['best_summary_loss']\n",
    "        self.epoch = checkpoint['epoch'] + 1\n",
    "        \n",
    "    def log(self, message):\n",
    "        if self.config.verbose:\n",
    "            print(message)\n",
    "        with open(self.log_path, 'a+') as logger:\n",
    "            logger.write(f'{message}\\n')\n",
    "\n",
    "\n",
    "# ## plabel 训练\n",
    "\n",
    "class TrainGlobalConfig:\n",
    "    num_workers = NUM_WORKERS\n",
    "    batch_size = 4 \n",
    "    n_epochs = 5 # n_epochs = 40\n",
    "    lr = 0.0001\n",
    "\n",
    "    folder = 'plabel_model'\n",
    "\n",
    "    # -------------------\n",
    "    verbose = True\n",
    "    verbose_step = 1\n",
    "    # -------------------\n",
    "\n",
    "    # --------------------\n",
    "    step_scheduler = False  # do scheduler.step after optimizer.step\n",
    "    validation_scheduler = True  # do scheduler.step after validation stage loss\n",
    "\n",
    "    SchedulerClass = torch.optim.lr_scheduler.ReduceLROnPlateau\n",
    "    scheduler_params = dict(\n",
    "        mode='min',\n",
    "        factor=0.5,\n",
    "        patience=1,\n",
    "        verbose=False, \n",
    "        threshold=0.0001,\n",
    "        threshold_mode='abs',\n",
    "        cooldown=0, \n",
    "        min_lr=1e-8,\n",
    "        eps=1e-08\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data.sampler import SequentialSampler, RandomSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = load_net(r'effdet5-cutmix-augmix1/best-checkpoint-035epoch.bin',mode='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_training():\n",
    "    device = torch.device('cuda:0')\n",
    "    net.to(device)\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=TrainGlobalConfig.batch_size,\n",
    "        sampler=RandomSampler(train_dataset),\n",
    "        pin_memory=False,\n",
    "        drop_last=True,\n",
    "        num_workers=TrainGlobalConfig.num_workers,\n",
    "        collate_fn=collate_fn,\n",
    "    )\n",
    "    val_loader = DataLoader(\n",
    "        validation_dataset, \n",
    "        batch_size=TrainGlobalConfig.batch_size,\n",
    "        sampler=SequentialSampler(validation_dataset),\n",
    "        pin_memory=False,\n",
    "        shuffle=False,\n",
    "        num_workers=TrainGlobalConfig.num_workers,\n",
    "        collate_fn=collate_fn,\n",
    "    )\n",
    "\n",
    "    fitter = Fitter(model=net, device=device, config=TrainGlobalConfig)\n",
    "    fitter.fit(train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run training...\n",
      "Fitter prepared. Device is cuda:0\n",
      "\n",
      "2020-07-19T14:05:18.772782\n",
      "LR: 0.0001\n"
     ]
    },
    {
     "ename": "BrokenPipeError",
     "evalue": "[Errno 32] Broken pipe",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mBrokenPipeError\u001B[0m                           Traceback (most recent call last)",
      "\u001B[1;32m<ipython-input-30-8b5ac2eb04ab>\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      3\u001B[0m \u001B[1;32melse\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      4\u001B[0m     \u001B[0mprint\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m\"run training...\"\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m----> 5\u001B[1;33m     \u001B[0mrun_training\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[1;32m<ipython-input-27-b6f4d866f67e>\u001B[0m in \u001B[0;36mrun_training\u001B[1;34m()\u001B[0m\n\u001B[0;32m     23\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     24\u001B[0m     \u001B[0mfitter\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mFitter\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mmodel\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mnet\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mdevice\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mdevice\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mconfig\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mTrainGlobalConfig\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 25\u001B[1;33m     \u001B[0mfitter\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mfit\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mtrain_loader\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mval_loader\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[1;32m<ipython-input-25-c51ac5b522b6>\u001B[0m in \u001B[0;36mfit\u001B[1;34m(self, train_loader, validation_loader)\u001B[0m\n\u001B[0;32m     38\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     39\u001B[0m             \u001B[0mt\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mtime\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mtime\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 40\u001B[1;33m             \u001B[0msummary_loss\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mtrain_one_epoch\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mtrain_loader\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     41\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     42\u001B[0m             \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mlog\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34mf'[RESULT]: Train. Epoch: {self.epoch}, summary_loss: {summary_loss.avg:.5f}, time: {(time.time() - t):.5f}'\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m<ipython-input-25-c51ac5b522b6>\u001B[0m in \u001B[0;36mtrain_one_epoch\u001B[1;34m(self, train_loader)\u001B[0m\n\u001B[0;32m     88\u001B[0m         \u001B[0msummary_loss\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mAverageMeter\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     89\u001B[0m         \u001B[0mt\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mtime\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mtime\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 90\u001B[1;33m         \u001B[1;32mfor\u001B[0m \u001B[0mstep\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m(\u001B[0m\u001B[0mimages\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtargets\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mimage_ids\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;32min\u001B[0m \u001B[0menumerate\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mtrain_loader\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     91\u001B[0m             \u001B[1;32mif\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mconfig\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mverbose\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     92\u001B[0m                 \u001B[1;32mif\u001B[0m \u001B[0mstep\u001B[0m \u001B[1;33m%\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mconfig\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mverbose_step\u001B[0m \u001B[1;33m==\u001B[0m \u001B[1;36m0\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mc:\\users\\administrator\\pytorch_cu101\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001B[0m in \u001B[0;36m__iter__\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    277\u001B[0m             \u001B[1;32mreturn\u001B[0m \u001B[0m_SingleProcessDataLoaderIter\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    278\u001B[0m         \u001B[1;32melse\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 279\u001B[1;33m             \u001B[1;32mreturn\u001B[0m \u001B[0m_MultiProcessingDataLoaderIter\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    280\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    281\u001B[0m     \u001B[1;33m@\u001B[0m\u001B[0mproperty\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mc:\\users\\administrator\\pytorch_cu101\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001B[0m in \u001B[0;36m__init__\u001B[1;34m(self, loader)\u001B[0m\n\u001B[0;32m    717\u001B[0m             \u001B[1;31m#     before it starts, and __del__ tries to join but will get:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    718\u001B[0m             \u001B[1;31m#     AssertionError: can only join a started process.\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 719\u001B[1;33m             \u001B[0mw\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mstart\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    720\u001B[0m             \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_index_queues\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mappend\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mindex_queue\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    721\u001B[0m             \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_workers\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mappend\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mw\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mc:\\users\\administrator\\appdata\\local\\programs\\python\\python37\\Lib\\multiprocessing\\process.py\u001B[0m in \u001B[0;36mstart\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    110\u001B[0m                \u001B[1;34m'daemonic processes are not allowed to have children'\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    111\u001B[0m         \u001B[0m_cleanup\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 112\u001B[1;33m         \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_popen\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_Popen\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    113\u001B[0m         \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_sentinel\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_popen\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0msentinel\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    114\u001B[0m         \u001B[1;31m# Avoid a refcycle if the target function holds an indirect\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mc:\\users\\administrator\\appdata\\local\\programs\\python\\python37\\Lib\\multiprocessing\\context.py\u001B[0m in \u001B[0;36m_Popen\u001B[1;34m(process_obj)\u001B[0m\n\u001B[0;32m    221\u001B[0m     \u001B[1;33m@\u001B[0m\u001B[0mstaticmethod\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    222\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0m_Popen\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mprocess_obj\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 223\u001B[1;33m         \u001B[1;32mreturn\u001B[0m \u001B[0m_default_context\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mget_context\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mProcess\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_Popen\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mprocess_obj\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    224\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    225\u001B[0m \u001B[1;32mclass\u001B[0m \u001B[0mDefaultContext\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mBaseContext\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mc:\\users\\administrator\\appdata\\local\\programs\\python\\python37\\Lib\\multiprocessing\\context.py\u001B[0m in \u001B[0;36m_Popen\u001B[1;34m(process_obj)\u001B[0m\n\u001B[0;32m    320\u001B[0m         \u001B[1;32mdef\u001B[0m \u001B[0m_Popen\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mprocess_obj\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    321\u001B[0m             \u001B[1;32mfrom\u001B[0m \u001B[1;33m.\u001B[0m\u001B[0mpopen_spawn_win32\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mPopen\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 322\u001B[1;33m             \u001B[1;32mreturn\u001B[0m \u001B[0mPopen\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mprocess_obj\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    323\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    324\u001B[0m     \u001B[1;32mclass\u001B[0m \u001B[0mSpawnContext\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mBaseContext\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mc:\\users\\administrator\\appdata\\local\\programs\\python\\python37\\Lib\\multiprocessing\\popen_spawn_win32.py\u001B[0m in \u001B[0;36m__init__\u001B[1;34m(self, process_obj)\u001B[0m\n\u001B[0;32m     87\u001B[0m             \u001B[1;32mtry\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     88\u001B[0m                 \u001B[0mreduction\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mdump\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mprep_data\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mto_child\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 89\u001B[1;33m                 \u001B[0mreduction\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mdump\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mprocess_obj\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mto_child\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     90\u001B[0m             \u001B[1;32mfinally\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     91\u001B[0m                 \u001B[0mset_spawning_popen\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;32mNone\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mc:\\users\\administrator\\appdata\\local\\programs\\python\\python37\\Lib\\multiprocessing\\reduction.py\u001B[0m in \u001B[0;36mdump\u001B[1;34m(obj, file, protocol)\u001B[0m\n\u001B[0;32m     58\u001B[0m \u001B[1;32mdef\u001B[0m \u001B[0mdump\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mobj\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mfile\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mprotocol\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;32mNone\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     59\u001B[0m     \u001B[1;34m'''Replacement for pickle.dump() using ForkingPickler.'''\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 60\u001B[1;33m     \u001B[0mForkingPickler\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mfile\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mprotocol\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mdump\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mobj\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     61\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     62\u001B[0m \u001B[1;31m#\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mBrokenPipeError\u001B[0m: [Errno 32] Broken pipe"
     ]
    }
   ],
   "source": [
    "if len(os.listdir('../input/global-wheat-detection/test/'))<1:\n",
    "    pass\n",
    "else:\n",
    "    print(\"run training...\")\n",
    "    run_training()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**============prediction with pl_model and TTA===============**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = f'{TrainGlobalConfig.folder}/last-checkpoint1.bin'\n",
    "if not os.path.exists(weights):\n",
    "    weights = '../input/eff_checkpoint/bestv2.bin'\n",
    "print(\"weight path: \",weights)\n",
    "net = load_net(weights,mode='eval')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_prediction_string(boxes, scores):\n",
    "    pred_strings = []\n",
    "    for j in zip(scores, boxes):\n",
    "        pred_strings.append(\"{0:.4f} {1} {2} {3} {4}\".format(j[0], j[1][0], j[1][1], j[1][2], j[1][3]))\n",
    "    return \" \".join(pred_strings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "results = [] \n",
    "for j, (images, image_ids) in enumerate(data_loader): \n",
    "    print(\"image ids:\",image_ids) \n",
    "    predictions = make_tta_predictions(images)\n",
    "\n",
    "    #predictions = make_predictions(images)\n",
    "\n",
    "    i = 0\n",
    "    sample = images[i].permute(1, 2, 0).cpu().numpy()\n",
    "\n",
    "    boxes, scores, labels = run_wbf(predictions, image_index=i)\n",
    "\n",
    "    boxes = boxes.astype(np.int32).clip(min=0, max=511)\n",
    "\n",
    "    boxes = (boxes * 2).astype(np.int32).clip(min=0, max=1023)\n",
    "\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(16, 8))\n",
    "\n",
    "\n",
    "    for box in boxes:\n",
    "        box = box//2 # 1024-->512\n",
    "        # sample = cv2.cvtColor(sample,cv2.COLOR_BGR2RGB)\n",
    "        cv2.rectangle(sample, (box[0], box[1]), (box[2], box[3]), (1, 0, 0), 1)\n",
    "    ax.set_axis_off()\n",
    "    ax.imshow(sample)\n",
    "    sample=cv2.cvtColor(sample,cv2.COLOR_RGB2BGR)\n",
    "    cv2.imwrite(str(j) + '.jpg', sample * 255)\n",
    "\n",
    "    image_id = image_ids[i]\n",
    "\n",
    "    boxes[:, 2] = boxes[:, 2] - boxes[:, 0]\n",
    "    boxes[:, 3] = boxes[:, 3] - boxes[:, 1]\n",
    "\n",
    "    result = {\n",
    "        'image_id': image_id,\n",
    "        'PredictionString': format_prediction_string(boxes, scores)\n",
    "    }\n",
    "    results.append(result)\n",
    "test_df = pd.DataFrame(results, columns=['image_id', 'PredictionString']) \n",
    "test_df.to_csv('submission.csv', index=False) \n",
    "test_df.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}